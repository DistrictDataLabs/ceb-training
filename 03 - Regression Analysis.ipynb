{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis with Scikit-Learn\n",
    "\n",
    "**Linear Regression** fits a linear model to the data by adjusting a set of coefficients w to minimize the residual sum of squares between observed responses & prediction. \n",
    "\n",
    "Linear model: $y=X\\beta+\\epsilon$\n",
    "\n",
    "Objective function: $min_w \\sum (Xw -y)^2$\n",
    "\n",
    "Predictive model: $\\hat{y}(w,x)=w_0 + w_1x_1+...+w_px_p$\n",
    "\n",
    "Notation:\n",
    "\n",
    "- $y$  is the observed value\n",
    "- $X$  is the input variables\n",
    "- $\\beta$  is the set of coefficients\n",
    "- $\\epsilon$  is noise or randomness in observation\n",
    "- $w$  is the array of weights \n",
    "- $w_0$ is the ability to adjust the plane in space\n",
    "- $\\hat{y}$  is the predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import sklearn\n",
    "import requests \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fixtures \n",
    "GENDATA = os.path.join(\"data\", \"generated\")\n",
    "DATASET = \"dataset{}.txt\"\n",
    "TARGET  = \"target{}.txt\"\n",
    "COEFS   = \"coefs{}.txt\"\n",
    "\n",
    "def load_gendata(suffix=\"\"):\n",
    "    X = np.loadtxt(os.path.join(GENDATA, DATASET.format(suffix)))\n",
    "    y = np.loadtxt(os.path.join(GENDATA, TARGET.format(suffix)))\n",
    "    w = np.loadtxt(os.path.join(GENDATA, COEFS.format(suffix)))\n",
    "    return X,y,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y,w = load_gendata() # Sample data set \n",
    "Xc,yc,wc = load_gendata(\"-collin\") # Collinear data set \n",
    "Xd,yd,wd = load_gendata(\"-demo\") # Demo data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fix for 1D demo (for viz)\n",
    "Xd = Xd.reshape(Xd.shape[0], 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares\n",
    "\n",
    "- Keep adjusting parameters until minimum squared residuals (e.g. minimize some cost function). \n",
    "- Relies on the independence of the model terms\n",
    "- *multicollinearity*: two or more predictor variables in a multiple regression model are highly correlated, one can be linearly predicted from the others\n",
    "- If this happens, the estimate becomes sensitive to error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(Xd, yd)\n",
    "print(model)\n",
    "print(model.coef_)\n",
    "print(wd)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_model(X, y, model, w):\n",
    "    k = X.shape[1]\n",
    "    if k > 2 or k < 1:\n",
    "        raise ValueError(\"Cannot plot in more than 3D!\")\n",
    "    \n",
    "    \n",
    "    # Determine if 2D or 3D \n",
    "    fig = plt.figure()\n",
    "    if k == 2:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        # Scatter plot of points \n",
    "        ax.scatter(X[:,0], X[:,1], y)\n",
    "        \n",
    "        # Line plot of original model\n",
    "        xm, ym = np.meshgrid(np.linspace(X[:,0].min(), X[:,0].max()), np.linspace(X[:,1].min(), X[:,1].max()))\n",
    "        zm = w[0]*xm + w[1]*ym + bias \n",
    "        ax.plot_wireframe(xm, ym, zm, alpha=0.5, c='b')\n",
    "        \n",
    "        # Line plot of predicted model \n",
    "        zp = model.predict(np.append(xm, ym))\n",
    "        ax.plot_wireframe(xm, ym, zp, alpha=0.5, c='g')\n",
    "    \n",
    "    else:\n",
    "        ax = fig.add_subplot(111)\n",
    "        \n",
    "        # Scatter plot of points \n",
    "        ax.scatter(X, y)\n",
    "        \n",
    "        # Line plot of original model\n",
    "        Xm = np.linspace(X.min(), X.max())\n",
    "        Xm = Xm.reshape(Xm.shape[0], 1)\n",
    "        ym = np.dot(Xm, w)\n",
    "        ax.plot(Xm, ym, c='b')\n",
    "        \n",
    "        # Line plot of predicted model \n",
    "        yp = model.predict(Xm)\n",
    "        ax.plot(Xm, yp, c='g')\n",
    "\n",
    "    return ax \n",
    "\n",
    "draw_model(Xd, yd, model, wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.cross_validation import train_test_split as tts \n",
    "\n",
    "X_train, X_test, y_train, y_test = tts(X, y)\n",
    "model = LinearRegression() \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "r2 = model.score(X_test, y_test)\n",
    "\n",
    "print(\"MSE: {:0.3f} | R2: {:0.3f}\".format(mse, r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization \n",
    "\n",
    "- As we increase the complexity of the model we reduce the bias but increase the variance of the model. \n",
    "- Variance: the tendency for the model to fit to noise (randomness) -- overfit. \n",
    "- Introduce a parameter to penalize complexity in the function being minimized.\n",
    "\n",
    "\n",
    "**Vector Norm**\n",
    "\n",
    "Describes the length of the vector. \n",
    "\n",
    "- L1: sum of the absolute values of components\n",
    "- L2: euclidian distance from the origin \n",
    "- L‚àû: maximal absolute value component "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tts(Xc, yc)\n",
    "model = LinearRegression() \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "r2 = model.score(X_test, y_test)\n",
    "\n",
    "print(\"{}\\nMSE: {:0.3f} | R2: {:0.3f}\".format(model,mse, r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regularization \n",
    "\n",
    "Prevent overfit/collinearity by penalizing the size of coefficients - minimize the penalized residual sum of squares:\n",
    "\n",
    "Said another way, shrink the coefficients to zero.\n",
    "\n",
    "$min_w (||Xw-y||_2)^2 + (\\alpha||w||_2)^2$\n",
    "\n",
    "Where ùõº > 0 is complexity parameter that controls shrinkage. The larger ùõº, the more robust the model to collinearity.\n",
    "\n",
    "Alpha influences the the bias/variance tradeoff: the larger the ridge alpha, the higher the bias and the lower the variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge \n",
    "\n",
    "model = Ridge(alpha=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "r2 = model.score(X_test, y_test)\n",
    "\n",
    "print(\"{}\\nMSE: {:0.3f} | R2: {:0.3f}\".format(model, mse, r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO Regularization \n",
    "\n",
    "Reducing bias is one thing, but what if the coefficients are very sparse? E.g. the more dimensions we add, the more space goes into the model. \n",
    "\n",
    "Lasso prefers fewer parameters attempting to reduce the number of variables the solution depends on.\n",
    "\n",
    "$min_w \\frac{1}{2n}(\\sum{(Xw-w)^2+\\alpha ||w||_1}$\n",
    "\n",
    "- The term $\\alpha‚Äñw‚Äñ_1$ is the L1 norm, whereas in ridge we used the L2 norm, $\\alpha‚Äñw‚Äñ_2$.  \n",
    "- See also Least Angle Regression (LARS) as similar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso \n",
    "\n",
    "model = Lasso(alpha=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "r2 = model.score(X_test, y_test)\n",
    "\n",
    "print(\"{}\\nMSE: {:0.3f} | R2: {:0.3f}\".format(model, mse, r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet Regularization \n",
    "\n",
    "Model trained with both L1 and L2 prior as regularizer. \n",
    "\n",
    "This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. Can control the convex combination of L1 and L2 using a ratio parameter. \n",
    "\n",
    "Elastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\n",
    "\n",
    "A practical advantage of trading-off between Lasso and Ridge is it allows Elastic-Net to inherit some of Ridge‚Äôs stability under rotation.\n",
    "\n",
    "$min_w \\frac{1} {2n} ||Xw-y||_2^2 + \\alpha\\rho||w||_1 + \\frac{\\alpha(1-\\rho)} {2}||w||_2^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet \n",
    "\n",
    "model = ElasticNet(alpha=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "r2 = model.score(X_test, y_test)\n",
    "\n",
    "print(\"{}\\nMSE: {:0.3f} | R2: {:0.3f}\".format(model, mse, r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Alpha \n",
    "\n",
    "We can search for the best parameter using the ModelCV which is a form of Grid Search, but uses a more efficient form of leave-one-out cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n",
    "\n",
    "alphas = np.logspace(-10, -2, 200)\n",
    "\n",
    "ridge = RidgeCV(alphas=alphas)\n",
    "lasso = LassoCV(alphas=alphas)\n",
    "elnet = ElasticNetCV(alphas=alphas)\n",
    "\n",
    "for model in (ridge, lasso, elnet):\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "    r2 = model.score(X_test, y_test)\n",
    "\n",
    "    print(\"{}\\nAlpha: {:0.3f} | MSE: {:0.3f} | R2: {:0.3f}\".format(model, model.alpha_, mse, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = Ridge(fit_intercept=False)\n",
    "errors = []\n",
    "for alpha in alphas:\n",
    "    splits = tts(X, y, test_size=0.2)\n",
    "    X_train, X_test, y_train, y_test = splits\n",
    "    clf.set_params(alpha=alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    error = mean_squared_error(y_test, clf.predict(X_test))\n",
    "    errors.append(error)\n",
    "\n",
    "axe = plt.gca()\n",
    "axe.plot(alphas, errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression \n",
    "\n",
    "In order to do higher order polynomial regression, we can use linear models trained on nonlinear functions of data!\n",
    "\n",
    "- Speed of linear model computation\n",
    "- Fit a wider range of data or functions\n",
    "- But remember: polynomials aren‚Äôt the only functions to fit\n",
    "\n",
    "The way this works is via Pipelining. \n",
    "\n",
    "Consider the standard linear regression case:\n",
    "\n",
    "$\\hat{y}(w,x) = w_0 + \\sum_i^n{w_ix_i}$\n",
    "\n",
    "The quadratic case (polynomial degree = 2) is:\n",
    "\n",
    "$\\hat{y}(w,v,x) = w_0 + \\sum_i^n{w_ix_i} + \\sum_i^n{v_ix_i^2}$\n",
    "\n",
    "But this can just be seen as a new feature space:\n",
    "\n",
    "$z = [x_1,...,x_n,x_1^2,...,x_n^2]$\n",
    "\n",
    "And this feature space can be computed in a linear fashion. We just need some way to add our 2nd degree dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "model = Pipeline([\n",
    "    ('poly', PolynomialFeatures(2)), \n",
    "    ('ridge', RidgeCV(alphas=alphas)),\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "r2 = model.score(X_test, y_test)\n",
    "\n",
    "print(\"{}\\nMSE: {:0.3f} | R2: {:0.3f}\".format(model, mse, r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy Data Set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ENERGY = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_data(url, path='data'):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    name = os.path.basename(url)\n",
    "    with open(os.path.join(path, name), 'wb') as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "download_data(ENERGY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "energy   = pd.read_excel('data/ENB2012_data.xlsx', sep=\",\")\n",
    "energy.columns = ['compactness','surface_area','wall_area','roof_area','height',\\\n",
    "                  'orientation','glazing_area','distribution','heating_load','cooling_load']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "energy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "energy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "ax = scatter_matrix(energy, alpha=0.2, figsize=(9,9), diagonal='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "energy_features = energy.ix[:,0:8]\n",
    "energy_labels = energy.ix[:,8:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are features predictive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RandomizedLasso\n",
    "\n",
    "model = RandomizedLasso(alpha=0.1)\n",
    "model.fit(energy_features, energy_labels[\"heating_load\"])\n",
    "names = list(energy_features)\n",
    "\n",
    "print(\"Features sorted by their score:\")\n",
    "print(sorted(zip(map(lambda x: round(x, 4), model.scores_), \n",
    "                 names), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = RandomizedLasso(alpha=0.1)\n",
    "model.fit(energy_features, energy_labels[\"cooling_load\"])\n",
    "names = list(energy_features)\n",
    "\n",
    "print(\"Features sorted by their score:\")\n",
    "print(sorted(zip(map(lambda x: round(x, 4), model.scores_), \n",
    "                 names), reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Heating Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "heat_labels = energy.ix[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_and_evaluate(model, X, y):\n",
    "    X_train, X_test, y_train, y_test = tts(X, y)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "    r2 = model.score(X_test, y_test)\n",
    "\n",
    "    print(\"{}\\nMSE: {:0.3f} | R2: {:0.3f}\".format(model, mse, r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    RidgeCV(alphas=alphas),\n",
    "    LassoCV(alphas=alphas),\n",
    "    ElasticNetCV(alphas=alphas),\n",
    "    RandomForestRegressor(), \n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    fit_and_evaluate(model, energy_features, heat_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
