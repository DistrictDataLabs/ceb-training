{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models with Scikit-Learn\n",
    "\n",
    "**Classification** algorithms assign a discrete label to a vector, X based on the vector's elements. Unlike regression models, there are a wide array of classification models from probabilistic Bayesian and Logistic Regression models to non-parameteric methods like kNN and Random Forests to linear methods like SVMs and LDA. \n",
    "\n",
    "As before the basic methodology is to select the best model through trial and evaluation with cross-validation and F1 Scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using the IRIS data set - the classic classification data set. \n",
    "from sklearn.cross_validation import train_test_split as tts\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = tts(data.data, data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Parametric Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "print(classification_report(yhat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "print(classification_report(yhat, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabalistic Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "print(classification_report(yhat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "print(classification_report(yhat, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "print(classification_report(yhat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "model = LinearDiscriminantAnalysis()\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "print(classification_report(yhat, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Wheat Kernels by Physical Property\n",
    "\n",
    "Downloaded from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/seeds) on February 26, 2015. The first thing is to fully describe your data in a README file. The dataset description is as follows:\n",
    "\n",
    "- Data Set: Multivariate\n",
    "- Attribute: Real\n",
    "- Tasks: Classification, Clustering\n",
    "- Instances: 210\n",
    "- Attributes: 7\n",
    "\n",
    "### Data Set Information:\n",
    "\n",
    "The examined group comprised kernels belonging to three different varieties of wheat: Kama, Rosa and Canadian, 70 elements each, randomly selected for the experiment. High quality visualization of the internal kernel structure was detected using a soft X-ray technique. It is non-destructive and considerably cheaper than other more sophisticated imaging techniques like scanning microscopy or laser technology. The images were recorded on 13x18 cm X-ray KODAK plates. Studies were conducted using combine harvested wheat grain originating from experimental fields, explored at the Institute of Agrophysics of the Polish Academy of Sciences in Lublin.\n",
    "\n",
    "The data set can be used for the tasks of classification and cluster analysis.\n",
    "\n",
    "### Attribute Information:\n",
    "\n",
    "To construct the data, seven geometric parameters of wheat kernels were measured:\n",
    "\n",
    "1. area A,\n",
    "2. perimeter P,\n",
    "3. compactness C = 4*pi*A/P^2,\n",
    "4. length of kernel,\n",
    "5. width of kernel,\n",
    "6. asymmetry coefficient\n",
    "7. length of kernel groove.\n",
    "\n",
    "All of these parameters were real-valued continuous.\n",
    "\n",
    "### Relevant Papers:\n",
    "\n",
    "M. Charytanowicz, J. Niewczas, P. Kulczycki, P.A. Kowalski, S. Lukasik, S. Zak, 'A Complete Gradient Clustering Algorithm for Features Analysis of X-ray Images', in: Information Technologies in Biomedicine, Ewa Pietka, Jacek Kawa (eds.), Springer-Verlag, Berlin-Heidelberg, 2010, pp. 15-24.\n",
    "\n",
    "## Data Exploration \n",
    "\n",
    "In this section we will begin to explore the dataset to determine relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt\"\n",
    "\n",
    "def fetch_data(fname='data/wheat/seeds_dataset.txt'):\n",
    "    \"\"\"\n",
    "    Helper method to retreive the ML Repository dataset.\n",
    "    \"\"\"\n",
    "    response = requests.get(URL)\n",
    "    outpath  = os.path.abspath(fname)\n",
    "    with open(outpath, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    return outpath\n",
    "\n",
    "# Fetch the data if required\n",
    "# DATA = fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FEATURES  = [\n",
    "    \"area\",\n",
    "    \"perimeter\",\n",
    "    \"compactness\",\n",
    "    \"length\",\n",
    "    \"width\",\n",
    "    \"asymmetry\",\n",
    "    \"groove\",\n",
    "    \"label\"\n",
    "]\n",
    "\n",
    "LABEL_MAP = {\n",
    "    1: \"Kama\",\n",
    "    2: \"Rosa\",\n",
    "    3: \"Canadian\",\n",
    "}\n",
    "\n",
    "# Read the data into a DataFrame\n",
    "df = pd.read_csv(DATA, sep='\\s+', header=None, names=FEATURES)\n",
    "\n",
    "# Convert class labels into text\n",
    "for k,v in LABEL_MAP.items():\n",
    "    df.ix[df.label == k, 'label'] = v\n",
    "\n",
    "# Describe the dataset\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determine the shape of the data\n",
    "print(\"{} instances with {} features\\n\".format(*df.shape))\n",
    "\n",
    "# Determine the frequency of each class\n",
    "print(df.groupby('label')['label'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a scatter matrix of the dataframe features\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "scatter_matrix(df, alpha=0.2, figsize=(12, 12), diagonal='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import parallel_coordinates\n",
    "plt.figure(figsize=(12,12))\n",
    "parallel_coordinates(df, 'label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import radviz\n",
    "plt.figure(figsize=(12,12))\n",
    "radviz(df, 'label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction \n",
    "\n",
    "One way that we can structure our data for easy management is to save files on disk. The Scikit-Learn datasets are already structured this way, and when loaded into a `Bunch` (a class imported from the `datasets` module of Scikit-Learn) we can expose a data API that is very familiar to how we've trained on our toy datasets in the past. A `Bunch` object exposes some important properties:\n",
    "\n",
    "- **data**: array of shape `n_samples` * `n_features`\n",
    "- **target**: array of length `n_samples`\n",
    "- **feature_names**: names of the features\n",
    "- **target_names**: names of the targets\n",
    "- **filenames**: names of the files that were loaded\n",
    "- **DESCR**: contents of the readme\n",
    "\n",
    "**Note**: This does not preclude database storage of the data, in fact - a database can be easily extended to load the same `Bunch` API. Simply store the README and features in a dataset description table and load it from there. The filenames property will be redundant, but you could store a SQL statement that shows the data load. \n",
    "\n",
    "In order to manage our data set _on disk_, we'll structure our data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.base import Bunch\n",
    "\n",
    "DATA_DIR = os.path.abspath(os.path.join(\".\", \"..\", \"data\", \"wheat\"))\n",
    "\n",
    "# Show the contents of the data directory\n",
    "for name in os.listdir(DATA_DIR):\n",
    "    if name.startswith(\".\"): continue\n",
    "    print(\"- {}\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data(root=DATA_DIR):\n",
    "    # Construct the `Bunch` for the wheat dataset\n",
    "    filenames     = {\n",
    "        'meta': os.path.join(root, 'meta.json'),\n",
    "        'rdme': os.path.join(root, 'README.md'),\n",
    "        'data': os.path.join(root, 'seeds_dataset.txt'),\n",
    "    }\n",
    "\n",
    "    # Load the meta data from the meta json\n",
    "    with open(filenames['meta'], 'r') as f:\n",
    "        meta = json.load(f)\n",
    "        target_names  = meta['target_names']\n",
    "        feature_names = meta['feature_names']\n",
    "\n",
    "    # Load the description from the README. \n",
    "    with open(filenames['rdme'], 'r') as f:\n",
    "        DESCR = f.read()\n",
    "\n",
    "    # Load the dataset from the text file.\n",
    "    dataset = np.loadtxt(filenames['data'])\n",
    "\n",
    "    # Extract the target from the data\n",
    "    data   = dataset[:, 0:-1]\n",
    "    target = dataset[:, -1]\n",
    "\n",
    "    # Create the bunch object\n",
    "    return Bunch(\n",
    "        data=data,\n",
    "        target=target,\n",
    "        filenames=filenames,\n",
    "        target_names=target_names,\n",
    "        feature_names=feature_names,\n",
    "        DESCR=DESCR\n",
    "    )\n",
    "\n",
    "# Save the dataset as a variable we can use.\n",
    "dataset = load_data()\n",
    "\n",
    "print(dataset.data.shape)\n",
    "print(dataset.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification \n",
    "\n",
    "Now that we have a dataset `Bunch` loaded and ready, we can begin the classification process. Let's attempt to build a classifier with kNN, SVM, and Random Forest classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_and_evaluate(dataset, model, label, **kwargs):\n",
    "    \"\"\"\n",
    "    Because of the Scikit-Learn API, we can create a function to\n",
    "    do all of the fit and evaluate work on our behalf!\n",
    "    \"\"\"\n",
    "    start  = time.time() # Start the clock! \n",
    "    scores = {'precision':[], 'recall':[], 'accuracy':[], 'f1':[]}\n",
    "    \n",
    "    for train, test in KFold(dataset.data.shape[0], n_folds=12, shuffle=True):\n",
    "        X_train, X_test = dataset.data[train], dataset.data[test]\n",
    "        y_train, y_test = dataset.target[train], dataset.target[test]\n",
    "        \n",
    "        estimator = model(**kwargs)\n",
    "        estimator.fit(X_train, y_train)\n",
    "        \n",
    "        expected  = y_test\n",
    "        predicted = estimator.predict(X_test)\n",
    "        \n",
    "        # Append our scores to the tracker\n",
    "        scores['precision'].append(metrics.precision_score(expected, predicted, average=\"weighted\"))\n",
    "        scores['recall'].append(metrics.recall_score(expected, predicted, average=\"weighted\"))\n",
    "        scores['accuracy'].append(metrics.accuracy_score(expected, predicted))\n",
    "        scores['f1'].append(metrics.f1_score(expected, predicted, average=\"weighted\"))\n",
    "\n",
    "    # Report\n",
    "    print(\"Build and Validation of {} took {:0.3f} seconds\".format(label, time.time()-start))\n",
    "    print(\"Validation scores are as follows:\\n\")\n",
    "    print(pd.DataFrame(scores).mean())\n",
    "    \n",
    "    # Write official estimator to disk\n",
    "    estimator = model(**kwargs)\n",
    "    estimator.fit(dataset.data, dataset.target)\n",
    "    \n",
    "    outpath = label.lower().replace(\" \", \"-\") + \".pickle\"\n",
    "    with open(outpath, 'wb') as f:\n",
    "        pickle.dump(estimator, f)\n",
    "\n",
    "    print(\"\\nFitted model written to:\\n{}\".format(os.path.abspath(outpath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform SVC Classification\n",
    "fit_and_evaluate(dataset, SVC, \"Wheat SVM Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform kNN Classification\n",
    "fit_and_evaluate(dataset, KNeighborsClassifier, \"Wheat kNN Classifier\", n_neighbors=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform Random Forest Classification\n",
    "fit_and_evaluate(dataset, RandomForestClassifier, \"Wheat Random Forest Classifier\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
