{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "In this notebook, we'll walk through some simple natural language processing techniques and work towards building a text classification model. Through this process we'll utilize the data science pipeline:\n",
    "\n",
    "Ingestion &rarr; Wrangling &rarr; Analysis &rarr; Modeling &rarr; Visualization\n",
    "\n",
    "The basic principle will be to fetch HTML data from web pages, then extract the text from it. We will then apply tokenization and tagging to the text to create a basic data structure. In preparation for modeling we'll normalize our text using lemmatization, then remove stopwords and punctuation. After that we'll vectorize our text, then send it to our classification model, which we will evaluate with cross validation. \n",
    "\n",
    "## Preprocessing Text\n",
    "\n",
    "\n",
    "### Step One: Fetch Data\n",
    "\n",
    "For now, we'll simply ingest news articles from the Washington Post by looking up their ID from the short URL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import requests \n",
    "\n",
    "WAPO = \"http://wpo.st/\"\n",
    "\n",
    "def fetch_wapo(sid=\"ciSa2\"):\n",
    "    url = WAPO + sid \n",
    "    res = requests.get(url) \n",
    "    return res.text\n",
    "\n",
    "story = fetch_wapo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Two: Clean Up Data \n",
    "\n",
    "The HTML that we fetched contains navigation, advertisements, and markup not related to the text. We need to clean it up to extract only the part of the document we're interested in analyzing. \n",
    "\n",
    "Note that this is also the point that we should consider larger document structures like chapters, sections, or paragraphs. If we want to consider paragraphs, the `extract` function should return a list of strings that each represent a paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from readability.readability import Document\n",
    "\n",
    "def extract(html):\n",
    "    article = Document(html).summary()\n",
    "    soup = BeautifulSoup(article, 'lxml')\n",
    "    \n",
    "    return soup.get_text()\n",
    "\n",
    "story = extract(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Three: Tokenization\n",
    "\n",
    "Tokenizers break down the text into units of logical meaning - sentences and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "\n",
    "def tokenize(text):\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        yield list(nltk.word_tokenize(sent))\n",
    "\n",
    "story = list(tokenize(story))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sent in story: print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Four: Tag Text \n",
    "\n",
    "Tagging adds information to the data structure we have -- namely the word class for each word (e.g. is it a Noun, Verb, Adjective, etc.). Note that tagging needs a complete sentence to work effectively. \n",
    "\n",
    "After we have tagged our text, we have completed the non-destructive operations on our text string, it is at this point that the text should be saved as a pickle to disk for use in downstream processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag(sents):\n",
    "    for sent in sents:\n",
    "        yield list(nltk.pos_tag(sent))\n",
    "\n",
    "story = list(tag(story))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sent in story: print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Normalize \n",
    "\n",
    "Normalization reduces the number of tokens that we pass to our analysis, allowing us to do more effective language inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def tagwn(tag):\n",
    "    return {\n",
    "        'N': wn.NOUN,\n",
    "        'V': wn.VERB,\n",
    "        'R': wn.ADV,\n",
    "        'J': wn.ADJ\n",
    "    }.get(tag[0], wn.NOUN)\n",
    "\n",
    "\n",
    "def lemmatize(tagged_sents):\n",
    "    for sent in tagged_sents:\n",
    "        for token, tag in sent:\n",
    "            yield lemmatizer.lemmatize(token, tagwn(tag))\n",
    "\n",
    "\n",
    "story = list(lemmatize(story))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "punctuation = set(punctuation)\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def normalize(tokens):\n",
    "    for token in tokens:\n",
    "        token = token.lower()\n",
    "        if not all(char in punctuation for char in token):\n",
    "            if token not in stopwords:\n",
    "                yield token\n",
    "        \n",
    "\n",
    "story = list(normalize(story))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Corpus\n",
    "\n",
    "Building models requires gathering multiple documents and performing the processing steps on them that we showed above. We've used a tool called [Baleen](http://baleen.districtdatalabs.com/) to ingest data from RSS feeds for the past year. (It currently contains 1,154,100 posts for 373 feeds after 5,566 jobs). \n",
    "\n",
    "We've provided a small sample of the corpus to start playing with the tool. It has saved documents in the following structure:\n",
    "\n",
    "- Each file stored in the directory of its category \n",
    "- One document per file, stored as a pickle \n",
    "- Document is a list of paragraphs \n",
    "- Paragraph is a list of sentences \n",
    "- Sentence is a list of (token, tag) tuples\n",
    "\n",
    "We can then create a reader to automatically fetch data from our corpus. This is a bit more complex, but necessary. Also note that we add our normalization process here as well, just so we don't have to repeat steps later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import pickle \n",
    "\n",
    "from nltk.corpus.reader.api import CorpusReader\n",
    "from nltk.corpus.reader.api import CategorizedCorpusReader\n",
    "\n",
    "CORPUS_PATH = \"data/baleen_sample\"\n",
    "PKL_PATTERN = r'(?!\\.)[a-z_\\s]+/[a-f0-9]+\\.pickle'\n",
    "CAT_PATTERN = r'([a-z_\\s]+)/.*'\n",
    "\n",
    "class PickledCorpus(CategorizedCorpusReader, CorpusReader):\n",
    "    \n",
    "    def __init__(self, root, fileids=PKL_PATTERN, cat_pattern=CAT_PATTERN):\n",
    "        CategorizedCorpusReader.__init__(self, {\"cat_pattern\": cat_pattern})\n",
    "        CorpusReader.__init__(self, root, fileids)\n",
    "        \n",
    "        self.punct = set(string.punctuation) | {'“', '—', '’', '”', '…'}\n",
    "        self.stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "        self.wordnet = nltk.WordNetLemmatizer() \n",
    "    \n",
    "    def _resolve(self, fileids, categories):\n",
    "        if fileids is not None and categories is not None:\n",
    "            raise ValueError(\"Specify fileids or categories, not both\")\n",
    "\n",
    "        if categories is not None:\n",
    "            return self.fileids(categories=categories)\n",
    "        \n",
    "        if fileids is None:\n",
    "            return self.fileids() \n",
    "        \n",
    "        return fileids\n",
    "    \n",
    "    def lemmatize(self, token, tag):\n",
    "        token = token.lower()\n",
    "        \n",
    "        if token not in self.stopwords:\n",
    "            if not all(c in self.punct for c in token):\n",
    "                tag =  {\n",
    "                    'N': wn.NOUN,\n",
    "                    'V': wn.VERB,\n",
    "                    'R': wn.ADV,\n",
    "                    'J': wn.ADJ\n",
    "                }.get(tag[0], wn.NOUN)\n",
    "                return self.wordnet.lemmatize(token, tag)\n",
    "    \n",
    "    def tokenize(self, doc):\n",
    "        # Expects a preprocessed document, removes stopwords and punctuation\n",
    "        # makes all tokens lowercase and lemmatizes them. \n",
    "        return list(filter(None, [\n",
    "            self.lemmatize(token, tag)\n",
    "            for paragraph in doc \n",
    "            for sentence in paragraph \n",
    "            for token, tag in sentence \n",
    "        ]))\n",
    "    \n",
    "    def docs(self, fileids=None, categories=None):\n",
    "        # Resolve the fileids and the categories\n",
    "        fileids = self._resolve(fileids, categories)\n",
    "\n",
    "        # Create a generator, loading one document into memory at a time.\n",
    "        for path, enc, fileid in self.abspaths(fileids, True, True):\n",
    "            with open(path, 'rb') as f:\n",
    "                yield self.tokenize(pickle.load(f))\n",
    "    \n",
    "    def labels(self, fileids=None, categories=None):\n",
    "        fileids = self._resolve(fileids, categories)\n",
    "        for fid in fileids:\n",
    "            yield self.categories(fid)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = PickledCorpus('data/baleen_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"{} documents in {} categories\".format(len(corpus.fileids()), len(corpus.categories())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import ConditionalFreqDist\n",
    "\n",
    "words = ConditionalFreqDist()\n",
    "\n",
    "for doc, label in zip(corpus.docs(), corpus.labels()):\n",
    "    for word in doc:\n",
    "        words[label][word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for label, counts in words.items():\n",
    "    print(\"{}: {:,} vocabulary and {:,} words\".format(\n",
    "        label, len(counts), sum(counts.values())\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a Corpus \n",
    "\n",
    "TSNE - stochastic neighbor embedding, is a useful mechanism for performing high dimensional data visualization on text. We will use our classes to try to visualize groupings of documents on a per-class basis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.decomposition import TruncatedSVD \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "cluster = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=lambda x: x, preprocessor=None, lowercase=False)), \n",
    "        ('svd', TruncatedSVD(n_components=50)), \n",
    "        ('tsne', TSNE(n_components=2))\n",
    "    ])\n",
    "\n",
    "docs = cluster.fit_transform(list(corpus.docs()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from collections import defaultdict \n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('notebook')\n",
    "\n",
    "colors = {\n",
    "    \"design\": \"#e74c3c\",\n",
    "    \"tech\": \"#3498db\",\n",
    "    \"business\": \"#27ae60\",\n",
    "    \"gaming\": \"#f1c40f\",\n",
    "    \"politics\": \"#2c3e50\",\n",
    "    \"news\": \"#bdc3c7\",\n",
    "    \"cooking\": \"#d35400\",\n",
    "    \"data_science\": \"#1abc9c\",\n",
    "    \"sports\": \"#e67e22\",\n",
    "    \"cinema\": \"#8e44ad\",\n",
    "    \"books\": \"#c0392b\",\n",
    "    \"do_it_yourself\": \"#34495e\",\n",
    "}\n",
    "\n",
    "series = defaultdict(lambda: {'x':[], 'y':[]})\n",
    "for idx, label in enumerate(corpus.labels()):\n",
    "    x, y = docs[idx]\n",
    "    series[label]['x'].append(x)\n",
    "    series[label]['y'].append(y)\n",
    "\n",
    "    \n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = plt.subplot(111)\n",
    "    \n",
    "for label, points in series.items():\n",
    "    ax.scatter(points['x'], points['y'], c=colors[label], alpha=0.7, label=label)\n",
    "\n",
    "# Add a title \n",
    "plt.title(\"TSNE Projection of the Baleen Corpus\")\n",
    "    \n",
    "# Remove the ticks \n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "\n",
    "# Add the legend \n",
    "# Shrink current axis by 20%\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "\n",
    "# Put a legend to the right of the current axis\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model \n",
    "\n",
    "We'll build a model that can classify what hobby a document is about based on our sample corpus.\n",
    "\n",
    "We'll need to add transformers that vectorize our text, and send them to a classification model. \n",
    "\n",
    "In this case we will evaluate with 12-part cross validation, using the `cross_val_predict` function and the `classifier_report` function.\n",
    "\n",
    "The function `cross_val_predict` has a similar interface to `cross_val_score`, but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set. Only cross-validation strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hobbies = ['gaming', 'cooking', 'sports', 'cinema', 'books', 'do_it_yourself']\n",
    "\n",
    "X = list(corpus.docs(categories=hobbies))\n",
    "y = list(corpus.labels(categories=hobbies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Models \n",
    "from sklearn.linear_model import SGDClassifier \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Transformers \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "# Evaluation \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def identity(words): \n",
    "    return words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SVM Classifier \n",
    "svm = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(tokenizer=identity, preprocessor=None, lowercase=False)), \n",
    "        ('svm', SGDClassifier()), \n",
    "    ])\n",
    "\n",
    "yhat = cross_val_predict(svm, X, y, cv=12)\n",
    "print(classification_report(y, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Logistic Regression \n",
    "logit = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(tokenizer=identity, preprocessor=None, lowercase=False)), \n",
    "        ('logit', LogisticRegression()), \n",
    "    ])\n",
    "\n",
    "yhat = cross_val_predict(logit, X, y, cv=12)\n",
    "print(classification_report(y, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "nbayes = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(tokenizer=identity, preprocessor=None, lowercase=False)), \n",
    "        ('nbayes', MultinomialNB()), \n",
    "    ])\n",
    "\n",
    "yhat = cross_val_predict(nbayes, X, y, cv=12)\n",
    "print(classification_report(y, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random Forest \n",
    "trees = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(tokenizer=identity, preprocessor=None, lowercase=False)), \n",
    "        ('trees', RandomForestClassifier()), \n",
    "    ])\n",
    "\n",
    "yhat = cross_val_predict(trees, X, y, cv=12)\n",
    "print(classification_report(y, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operationalization \n",
    "\n",
    "At this point we can save our best performing model to disk and use it to classify new text. \n",
    "\n",
    "The most important thing to remember is that the input to our model needs to be identical to the input we trained our model upon. Because we preprocessed our text in the experimental phase, we have to preprocess it before we make predictions on it as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(path, corpus):\n",
    "    model = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(tokenizer=identity, preprocessor=None, lowercase=False)), \n",
    "        ('svm', SGDClassifier(loss='log')), \n",
    "    ])\n",
    "    \n",
    "    # Train model on the entire data set \n",
    "    X = list(corpus.docs(categories=hobbies))\n",
    "    y = list(corpus.labels(categories=hobbies))\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "build_model('data/hobbies.classifier', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can now load our model from disk \n",
    "with open('data/hobbies.classifier', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a normalization method for fetching URL content\n",
    "# that our model expects, based on our methods above. \n",
    "def fetch(url):\n",
    "    html = requests.get(url)\n",
    "    text = extract(html.text)\n",
    "    tokens = tokenize(text)\n",
    "    tags = tag(tokens)\n",
    "    lemmas = lemmatize(tags)\n",
    "    return list(normalize(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(url):\n",
    "    text = fetch(url)\n",
    "    probs = zip(model.classes_, model.predict_proba([text])[0])\n",
    "    label = model.predict([text])[0]\n",
    "    \n",
    "    print(\"y={}\".format(label))\n",
    "    for cls, prob in sorted(probs, key=lambda x: x[1]):\n",
    "        print(\"  {}: {:0.3f}\".format(cls, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict(\"http://minimalistbaker.com/5-ingredient-white-chocolate-truffles/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
